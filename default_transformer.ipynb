{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper Parameters"
      ],
      "metadata": {
        "id": "fhzxJxgtjzrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 40000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "score_size=3\n",
        "# ------------\n",
        "feel_count=16\n",
        "feel_embd=32\n",
        "# ------------\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "0TdMs5CMjIvY",
        "outputId": "2f9a77b5-b391-4ad8-f47b-b07c8e758429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d8ef1375b50>"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "Q5Yk9RulkNbo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgJN0y6_IB-X",
        "outputId": "52b7985e-7510-4927-8676-da78ccd8a007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-17 01:49:53--  https://raw.githubusercontent.com/BlastOfMihh/TennisMania/refs/heads/model/tt%20csv%20artificial.CSV\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3554 (3.5K) [text/plain]\n",
            "Saving to: ‘tt csv artificial.CSV.25’\n",
            "\n",
            "\rtt csv artificial.C   0%[                    ]       0  --.-KB/s               \rtt csv artificial.C 100%[===================>]   3.47K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-17 01:49:53 (29.2 MB/s) - ‘tt csv artificial.CSV.25’ saved [3554/3554]\n",
            "\n",
            "75 124 49\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "#drive.mount('/content/drive')\n",
        "#with open(\"/content/drive/MyDrive/LLm fighthing/karamazov.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "#  text=f.read()\n",
        "\n",
        "!wget \"https://raw.githubusercontent.com/BlastOfMihh/TennisMania/refs/heads/model/tt%20csv%20artificial.CSV\"\n",
        "one_csv_path=\"./tt csv artificial.CSV\"\n",
        "\n",
        "df=pd.read_csv(one_csv_path)\n",
        "df = df.drop(df.columns[0], axis=1)\n",
        "df\n",
        "\n",
        "hb_column = df.iloc[:, 0]\n",
        "min_hb = hb_column.min()\n",
        "max_hb = hb_column.max()\n",
        "diff_hb = max_hb - min_hb + 1\n",
        "print(min_hb, max_hb, max_hb-min_hb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "source": [
        "first_column = df.iloc[:, 0]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "hjQvmqwag_Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the dataset, Encoder, Decoder\n"
      ],
      "metadata": {
        "id": "0RAxnyLIegE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "#chars = sorted(list(set(text)))\n",
        "# chars=[]\n",
        "# heart_beat_min = df.iloc[:, 0].min()\n",
        "# heart_beat_max = df.iloc[:, 0].max()\n",
        "# for hb in range(heart_beat_min, heart_beat_max):\n",
        "#   for p1 in range(0,12):\n",
        "#     for p1 in range(0,12):\n",
        "#       chars.append()\n",
        "#\n",
        "# def encode_(hb, p1, p2):\n",
        "#   return hb*1000+p2*100+p1\n",
        "#\n",
        "#\n",
        "# for i in range(60)\n",
        "#\n",
        "# vocab_size = len(chars)\n",
        "# # create a mapping from characters to integers\n",
        "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "# itos = { i:ch for i,ch in enumerate(chars) }\n",
        "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "#data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "csv_tensor=torch.tensor(df.values)\n",
        "\n",
        "\n",
        "data = torch.stack([csv_tensor]*100, dim=0).to(device)\n",
        "\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def pick_block(x):\n",
        "  start=torch.randint(0, len(x)-block_size+1 -1, (1,))\n",
        "  return x[start:start+block_size-1],  x[start+1:start+block_size-1+1]\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    ix = torch.randint(0, len(data), (batch_size,))\n",
        "\n",
        "    data_blocks=[pick_block(data[i]) for i in ix]\n",
        "    x=torch.stack([block[0] for block in data_blocks], dim=0).to(device)\n",
        "    y=torch.stack([block[1] for block in data_blocks], dim=0).to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "get_batch('train')[0].shape\n",
        "\n"
      ],
      "metadata": {
        "id": "wm1JwxmLehx0",
        "outputId": "970a6b14-3303-4ed4-e515-51af3e052a1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 31, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F Model\n"
      ],
      "metadata": {
        "id": "HU12R4dqeqOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The feel stuff\n",
        "class FeelHead(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.key = nn.Linear(feel_embd, head_size, bias=False)\n",
        "\n",
        "        self.value = nn.Linear(feel_embd, head_size, bias=False)\n",
        "\n",
        "        self.feelings = torch.rand(feel_count, feel_embd).to(device)\n",
        "\n",
        "        #self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        q = self.query(x) # (B,T,C)\n",
        "\n",
        "        k = self.key(self.feelings) # (f_cnt, C)\n",
        "        k = k.expand(B, -1,-1) # (B, F_cnt, C)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, F_cnt) -> (B, T, F_cnt)\n",
        "        #wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, )\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(self.feelings) # (f_cnt, C)\n",
        "        v = v.expand(B,-1,-1) # (B, F_cnt, C)\n",
        "        out = wei @ v # (B, T, F_cnt) @ (B, F_cnt, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadFeeling(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([FeelHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "ynNnlYyEi1bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Regular"
      ],
      "metadata": {
        "id": "Je_X6PsN0qk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, model_type):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        self.model_type=model_type\n",
        "        if model_type=='feel':\n",
        "          self.mh_feel=MultiHeadFeeling(n_head, head_size)\n",
        "          self.ln3 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        if self.model_type=='feel':\n",
        "          x = x + self.mh_feel(self.ln3(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, model_type):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Linear(score_size, n_embd) # changed this\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, model_type=model_type) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, diff_hb+12+12)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T, _= idx.shape\n",
        "        idx=idx.to(torch.float32)\n",
        "        # idx and targets are both (B,T,3) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,3)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "\n",
        "            logits_hb = logits[:, :, :diff_hb]\n",
        "            logits_p1 = logits[:, :, diff_hb:diff_hb+12]\n",
        "            logits_p2 = logits[:, :, diff_hb+12:diff_hb+12+12]\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            logits_hb = logits[:, :, :diff_hb].view(B*T, diff_hb)\n",
        "            logits_p1 = logits[:, :, diff_hb:diff_hb+12].view(B*T, 12)\n",
        "            logits_p2 = logits[:, :, diff_hb+12:diff_hb+12+12].view(B*T, 12)\n",
        "\n",
        "            targets_hb = targets[:, :, 0:1].view(B*T)\n",
        "            targets_p1 = targets[:, :, 1:2].view(B*T)\n",
        "            targets_p2 = targets[:, :, 2:3].view(B*T)\n",
        "\n",
        "            loss_hb = F.cross_entropy(logits_hb, targets_hb-torch.full((B*T,),min_hb))\n",
        "            loss_p1 = F.cross_entropy(logits_p1, targets_p1)\n",
        "            loss_p2 = F.cross_entropy(logits_p2, targets_p2)\n",
        "\n",
        "            loss = loss_hb + loss_p1 + loss_p2\n",
        "\n",
        "\n",
        "#            logits = logits.view(B*T, C)\n",
        "#            targets = targets.view(B*T)\n",
        "\n",
        "            #loss =  F.mse_loss(logits, targets) # changed this ?\n",
        "            #loss = F.cross_entropy(logits, targets)\n",
        "        return (logits_hb, logits_p1, logits_p2), loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            (logits_hb, logits_p1, logits_p2), loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits_hb = logits_hb[:, -1, :] # becomes (B, C)\n",
        "            logits_p1 = logits_p1[:, -1, :] # becomes (B, C)\n",
        "            logits_p2 = logits_p2[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs_hb = F.softmax(logits_hb, dim=-1) # (B, C)\n",
        "            probs_p1 = F.softmax(logits_p1, dim=-1) # (B, C)\n",
        "            probs_p2 = F.softmax(logits_p2, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            hb_next = torch.multinomial(probs_hb, num_samples=1) # (B, 1)\n",
        "            p1_next = torch.multinomial(probs_p1, num_samples=1) # (B, 1)\n",
        "            p2_next = torch.multinomial(probs_p2, num_samples=1) # (B, 1)\n",
        "\n",
        "            idx_next = torch.stack([hb_next+min_hb,p1_next,p2_next], dim=-1)\n",
        "            # breakpoint()\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model_sdf = BigramLanguageModel('regular').to(device)\n",
        "\n",
        "idx=torch.stack([torch.stack([torch.tensor([85.0, 0.0, 0.0])]).to(device)])\n",
        "model_sdf.generate(idx, 10)\n",
        "\n",
        "#m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "#print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n"
      ],
      "metadata": {
        "id": "7hIAkhOHenNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2d8a70-1bcc-4aab-94fe-0aa91515c68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[85.,  0.,  0.],\n",
              "         [84.,  0.,  0.],\n",
              "         [84.,  0.,  0.],\n",
              "         [84.,  0.,  0.],\n",
              "         [75.,  0.,  0.],\n",
              "         [83.,  0.,  0.],\n",
              "         [86.,  0.,  0.],\n",
              "         [86.,  0.,  0.],\n",
              "         [85.,  0.,  0.],\n",
              "         [80.,  0.,  0.],\n",
              "         [75.,  0.,  0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and loss estimation functions\n"
      ],
      "metadata": {
        "id": "lgcpT2pWe5OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "def get_optimizer(model):\n",
        "  return torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_model(model, optimizer, history):\n",
        "  for iter in range(max_iters):\n",
        "\n",
        "      # every once in a while evaluate the loss on train and val sets\n",
        "      if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "          losses = estimate_loss(model)\n",
        "          print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "          history['train'].append(losses['train'])\n",
        "          history['val'].append(losses['val'])\n",
        "\n",
        "      # sample a batch of data\n",
        "      xb, yb = get_batch('train')\n",
        "\n",
        "      # evaluate the loss\n",
        "      _, loss = model(xb, yb)\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "def generate_from_model(m):\n",
        "  # generate from the model\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  print((m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "otjwJ-5Be9HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plotting and comparisions"
      ],
      "metadata": {
        "id": "hQ3Us6N-fn1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_regular={\n",
        "  'train':[],\n",
        "  'val':[]\n",
        "}\n",
        "history_feel={\n",
        "  'train':[],\n",
        "  'val':[]\n",
        "}"
      ],
      "metadata": {
        "id": "StHzo3IFfRWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_regular = BigramLanguageModel('regular').to(device)\n",
        "model_feel = BigramLanguageModel('feel').to(device)"
      ],
      "metadata": {
        "id": "URITk4f_g01v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_regular=get_optimizer(model_regular)\n",
        "#optimizer_feel=get_optimizer(model_feel)"
      ],
      "metadata": {
        "id": "vtyTCqg6hWPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_model(model_feel, optimizer_feel, history_feel)\n",
        "train_model(model_regular, optimizer_regular, history_regular)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM7lA73uhhNt",
        "outputId": "5bbec0c0-4947-41da-9ef7-04e87292808c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.1354, val loss 9.0945\n",
            "step 100: train loss 6.9627, val loss 6.9601\n",
            "step 200: train loss 6.5451, val loss 6.5434\n",
            "step 300: train loss 5.0614, val loss 5.0702\n",
            "step 400: train loss 4.5413, val loss 4.5532\n",
            "step 500: train loss 4.2720, val loss 4.2870\n",
            "step 600: train loss 4.0898, val loss 4.0715\n",
            "step 700: train loss 3.8115, val loss 3.7791\n",
            "step 800: train loss 3.5839, val loss 3.6242\n",
            "step 900: train loss 3.5485, val loss 3.5801\n",
            "step 1000: train loss 3.4030, val loss 3.3934\n",
            "step 1100: train loss 3.5458, val loss 3.5329\n",
            "step 1200: train loss 3.5258, val loss 3.5474\n",
            "step 1300: train loss 3.3656, val loss 3.3263\n",
            "step 1400: train loss 3.1389, val loss 3.0905\n",
            "step 1500: train loss 3.2854, val loss 3.2658\n",
            "step 1600: train loss 3.2283, val loss 3.2155\n",
            "step 1700: train loss 3.0029, val loss 3.0066\n",
            "step 1800: train loss 3.4186, val loss 3.4265\n",
            "step 1900: train loss 2.9170, val loss 2.9208\n",
            "step 2000: train loss 2.9900, val loss 3.0018\n",
            "step 2100: train loss 2.8349, val loss 2.8802\n",
            "step 2200: train loss 3.3320, val loss 3.3022\n",
            "step 2300: train loss 2.8820, val loss 2.9006\n",
            "step 2400: train loss 2.7388, val loss 2.7184\n",
            "step 2500: train loss 2.7281, val loss 2.6997\n",
            "step 2600: train loss 2.6643, val loss 2.6577\n",
            "step 2700: train loss 2.6691, val loss 2.7229\n",
            "step 2800: train loss 2.5809, val loss 2.5657\n",
            "step 2900: train loss 2.5949, val loss 2.5847\n",
            "step 3000: train loss 2.5035, val loss 2.4669\n",
            "step 3100: train loss 2.6621, val loss 2.6512\n",
            "step 3200: train loss 2.4722, val loss 2.4776\n",
            "step 3300: train loss 2.5660, val loss 2.5345\n",
            "step 3400: train loss 2.5434, val loss 2.5446\n",
            "step 3500: train loss 2.4451, val loss 2.4459\n",
            "step 3600: train loss 2.6178, val loss 2.6293\n",
            "step 3700: train loss 2.4497, val loss 2.4404\n",
            "step 3800: train loss 3.4576, val loss 3.4673\n",
            "step 3900: train loss 2.6061, val loss 2.5918\n",
            "step 4000: train loss 2.2847, val loss 2.3205\n",
            "step 4100: train loss 2.2210, val loss 2.2046\n",
            "step 4200: train loss 2.1891, val loss 2.1834\n",
            "step 4300: train loss 2.3937, val loss 2.3824\n",
            "step 4400: train loss 2.1851, val loss 2.1731\n",
            "step 4500: train loss 2.1102, val loss 2.1109\n",
            "step 4600: train loss 2.2967, val loss 2.2988\n",
            "step 4700: train loss 2.1819, val loss 2.1733\n",
            "step 4800: train loss 2.1133, val loss 2.0986\n",
            "step 4900: train loss 2.0799, val loss 2.0802\n",
            "step 5000: train loss 2.0185, val loss 2.0119\n",
            "step 5100: train loss 2.0164, val loss 2.0239\n",
            "step 5200: train loss 2.0390, val loss 2.0380\n",
            "step 5300: train loss 2.1855, val loss 2.1986\n",
            "step 5400: train loss 2.1313, val loss 2.1030\n",
            "step 5500: train loss 1.8914, val loss 1.8919\n",
            "step 5600: train loss 1.8322, val loss 1.8486\n",
            "step 5700: train loss 1.9513, val loss 1.9510\n",
            "step 5800: train loss 1.9146, val loss 1.9098\n",
            "step 5900: train loss 1.9005, val loss 1.9031\n",
            "step 6000: train loss 1.9113, val loss 1.9209\n",
            "step 6100: train loss 2.1693, val loss 2.1462\n",
            "step 6200: train loss 1.7265, val loss 1.7260\n",
            "step 6300: train loss 1.8241, val loss 1.8350\n",
            "step 6400: train loss 1.6732, val loss 1.6741\n",
            "step 6500: train loss 1.7603, val loss 1.7630\n",
            "step 6600: train loss 1.7130, val loss 1.7230\n",
            "step 6700: train loss 1.7297, val loss 1.6910\n",
            "step 6800: train loss 1.6264, val loss 1.6202\n",
            "step 6900: train loss 1.7821, val loss 1.7853\n",
            "step 7000: train loss 1.6452, val loss 1.6332\n",
            "step 7100: train loss 1.5373, val loss 1.5512\n",
            "step 7200: train loss 2.7448, val loss 2.7430\n",
            "step 7300: train loss 1.6220, val loss 1.6394\n",
            "step 7400: train loss 1.9773, val loss 1.9594\n",
            "step 7500: train loss 1.6608, val loss 1.6671\n",
            "step 7600: train loss 1.5259, val loss 1.5196\n",
            "step 7700: train loss 1.5798, val loss 1.5693\n",
            "step 7800: train loss 1.4838, val loss 1.4684\n",
            "step 7900: train loss 1.9647, val loss 1.9577\n",
            "step 8000: train loss 1.5580, val loss 1.5807\n",
            "step 8100: train loss 1.4718, val loss 1.4735\n",
            "step 8200: train loss 1.4428, val loss 1.4160\n",
            "step 8300: train loss 1.5110, val loss 1.5216\n",
            "step 8400: train loss 1.4364, val loss 1.4398\n",
            "step 8500: train loss 1.4730, val loss 1.4740\n",
            "step 8600: train loss 1.3862, val loss 1.3994\n",
            "step 8700: train loss 2.1060, val loss 2.1018\n",
            "step 8800: train loss 1.4086, val loss 1.4130\n",
            "step 8900: train loss 1.2843, val loss 1.2791\n",
            "step 9000: train loss 1.4516, val loss 1.4506\n",
            "step 9100: train loss 2.1677, val loss 2.1719\n",
            "step 9200: train loss 1.3783, val loss 1.3841\n",
            "step 9300: train loss 1.5151, val loss 1.5124\n",
            "step 9400: train loss 1.3921, val loss 1.3900\n",
            "step 9500: train loss 1.2935, val loss 1.2935\n",
            "step 9600: train loss 1.3874, val loss 1.3809\n",
            "step 9700: train loss 1.2592, val loss 1.2831\n",
            "step 9800: train loss 1.2153, val loss 1.2068\n",
            "step 9900: train loss 1.4194, val loss 1.4272\n",
            "step 10000: train loss 1.3358, val loss 1.3551\n",
            "step 10100: train loss 1.2701, val loss 1.2738\n",
            "step 10200: train loss 1.3565, val loss 1.3744\n",
            "step 10300: train loss 1.1889, val loss 1.1668\n",
            "step 10400: train loss 1.1797, val loss 1.1893\n",
            "step 10500: train loss 1.1436, val loss 1.1459\n",
            "step 10600: train loss 1.2062, val loss 1.1991\n",
            "step 10700: train loss 1.4774, val loss 1.4979\n",
            "step 10800: train loss 1.1237, val loss 1.1108\n",
            "step 10900: train loss 1.0843, val loss 1.0992\n",
            "step 11000: train loss 1.0656, val loss 1.0683\n",
            "step 11100: train loss 1.1425, val loss 1.1621\n",
            "step 11200: train loss 1.1859, val loss 1.1880\n",
            "step 11300: train loss 1.1310, val loss 1.1225\n",
            "step 11400: train loss 1.0194, val loss 1.0260\n",
            "step 11500: train loss 1.2811, val loss 1.2853\n",
            "step 11600: train loss 1.0344, val loss 1.0261\n",
            "step 11700: train loss 1.0254, val loss 1.0288\n",
            "step 11800: train loss 0.9663, val loss 0.9491\n",
            "step 11900: train loss 0.9788, val loss 0.9643\n",
            "step 12000: train loss 0.9282, val loss 0.9272\n",
            "step 12100: train loss 0.9294, val loss 0.9243\n",
            "step 12200: train loss 1.0357, val loss 1.0326\n",
            "step 12300: train loss 0.9581, val loss 0.9546\n",
            "step 12400: train loss 0.8472, val loss 0.8591\n",
            "step 12500: train loss 0.8945, val loss 0.9082\n",
            "step 12600: train loss 1.1044, val loss 1.0888\n",
            "step 12700: train loss 0.8672, val loss 0.8653\n",
            "step 12800: train loss 0.9863, val loss 0.9872\n",
            "step 12900: train loss 0.8508, val loss 0.8641\n",
            "step 13000: train loss 0.8795, val loss 0.8868\n",
            "step 13100: train loss 0.8844, val loss 0.8838\n",
            "step 13200: train loss 0.8363, val loss 0.8466\n",
            "step 13300: train loss 0.9013, val loss 0.8978\n",
            "step 13400: train loss 0.8146, val loss 0.8310\n",
            "step 13500: train loss 0.8204, val loss 0.8180\n",
            "step 13600: train loss 0.8281, val loss 0.8196\n",
            "step 13700: train loss 0.7566, val loss 0.7641\n",
            "step 13800: train loss 0.7631, val loss 0.7473\n",
            "step 13900: train loss 0.8240, val loss 0.8245\n",
            "step 14000: train loss 0.7489, val loss 0.7851\n",
            "step 14100: train loss 0.7564, val loss 0.7653\n",
            "step 14200: train loss 0.7454, val loss 0.7388\n",
            "step 14300: train loss 0.8377, val loss 0.8655\n",
            "step 14400: train loss 0.7315, val loss 0.7219\n",
            "step 14500: train loss 0.6962, val loss 0.6848\n",
            "step 14600: train loss 0.6346, val loss 0.6347\n",
            "step 14700: train loss 0.6793, val loss 0.6812\n",
            "step 14800: train loss 0.7326, val loss 0.7563\n",
            "step 14900: train loss 0.7976, val loss 0.7919\n",
            "step 15000: train loss 0.7467, val loss 0.7421\n",
            "step 15100: train loss 0.6337, val loss 0.6289\n",
            "step 15200: train loss 0.6519, val loss 0.6406\n",
            "step 15300: train loss 0.5724, val loss 0.5940\n",
            "step 15400: train loss 0.5855, val loss 0.5775\n",
            "step 15500: train loss 0.6030, val loss 0.5991\n",
            "step 15600: train loss 0.7121, val loss 0.7094\n",
            "step 15700: train loss 0.5808, val loss 0.5842\n",
            "step 15800: train loss 0.5925, val loss 0.6167\n",
            "step 15900: train loss 0.5867, val loss 0.5618\n",
            "step 16000: train loss 0.6015, val loss 0.5820\n",
            "step 16100: train loss 0.6990, val loss 0.7238\n",
            "step 16200: train loss 0.9167, val loss 0.9378\n",
            "step 16300: train loss 0.5385, val loss 0.5541\n",
            "step 16400: train loss 0.5261, val loss 0.5350\n",
            "step 16500: train loss 0.5362, val loss 0.5408\n",
            "step 16600: train loss 0.5092, val loss 0.5107\n",
            "step 16700: train loss 0.4932, val loss 0.4851\n",
            "step 16800: train loss 0.5429, val loss 0.5472\n",
            "step 16900: train loss 0.4950, val loss 0.4945\n",
            "step 17000: train loss 0.4952, val loss 0.5045\n",
            "step 17100: train loss 0.5954, val loss 0.5992\n",
            "step 17200: train loss 0.4752, val loss 0.4834\n",
            "step 17300: train loss 0.4880, val loss 0.4960\n",
            "step 17400: train loss 0.5010, val loss 0.4947\n",
            "step 17500: train loss 0.5064, val loss 0.5187\n",
            "step 17600: train loss 0.4466, val loss 0.4478\n",
            "step 17700: train loss 0.4177, val loss 0.4146\n",
            "step 17800: train loss 0.5556, val loss 0.5714\n",
            "step 17900: train loss 0.4077, val loss 0.3979\n",
            "step 18000: train loss 0.4518, val loss 0.4344\n",
            "step 18100: train loss 0.4542, val loss 0.4369\n",
            "step 18200: train loss 0.4026, val loss 0.4027\n",
            "step 18300: train loss 0.4216, val loss 0.4298\n",
            "step 18400: train loss 0.5270, val loss 0.4914\n",
            "step 18500: train loss 0.4362, val loss 0.4360\n",
            "step 18600: train loss 0.3762, val loss 0.3989\n",
            "step 18700: train loss 0.3994, val loss 0.3866\n",
            "step 18800: train loss 0.4171, val loss 0.4319\n",
            "step 18900: train loss 0.3654, val loss 0.3507\n",
            "step 19000: train loss 0.3916, val loss 0.3933\n",
            "step 19100: train loss 0.3394, val loss 0.3370\n",
            "step 19200: train loss 0.3363, val loss 0.3312\n",
            "step 19300: train loss 0.3747, val loss 0.3608\n",
            "step 19400: train loss 0.3952, val loss 0.3938\n",
            "step 19500: train loss 0.4775, val loss 0.4565\n",
            "step 19600: train loss 0.3621, val loss 0.3600\n",
            "step 19700: train loss 0.4241, val loss 0.4197\n",
            "step 19800: train loss 0.3299, val loss 0.3341\n",
            "step 19900: train loss 0.3669, val loss 0.3715\n",
            "step 20000: train loss 0.4546, val loss 0.4466\n",
            "step 20100: train loss 0.3688, val loss 0.3732\n",
            "step 20200: train loss 0.3117, val loss 0.3218\n",
            "step 20300: train loss 0.3693, val loss 0.3502\n",
            "step 20400: train loss 0.3993, val loss 0.4033\n",
            "step 20500: train loss 0.3287, val loss 0.3319\n",
            "step 20600: train loss 0.3181, val loss 0.3163\n",
            "step 20700: train loss 0.3037, val loss 0.3095\n",
            "step 20800: train loss 0.3275, val loss 0.3406\n",
            "step 20900: train loss 0.3658, val loss 0.3384\n",
            "step 21000: train loss 0.2986, val loss 0.2943\n",
            "step 21100: train loss 0.2826, val loss 0.2751\n",
            "step 21200: train loss 0.3169, val loss 0.3089\n",
            "step 21300: train loss 0.3244, val loss 0.3326\n",
            "step 21400: train loss 0.3030, val loss 0.3233\n",
            "step 21500: train loss 0.3329, val loss 0.3317\n",
            "step 21600: train loss 0.2884, val loss 0.2968\n",
            "step 21700: train loss 0.3112, val loss 0.3061\n",
            "step 21800: train loss 0.3197, val loss 0.3058\n",
            "step 21900: train loss 0.3254, val loss 0.3209\n",
            "step 22000: train loss 0.4490, val loss 0.4444\n",
            "step 22100: train loss 0.3315, val loss 0.3287\n",
            "step 22200: train loss 0.2516, val loss 0.2532\n",
            "step 22300: train loss 0.2293, val loss 0.2315\n",
            "step 22400: train loss 0.2873, val loss 0.2759\n",
            "step 22500: train loss 0.2455, val loss 0.2463\n",
            "step 22600: train loss 0.2494, val loss 0.2421\n",
            "step 22700: train loss 0.2636, val loss 0.2648\n",
            "step 22800: train loss 0.3190, val loss 0.3168\n",
            "step 22900: train loss 0.2261, val loss 0.2357\n",
            "step 23000: train loss 0.3182, val loss 0.3259\n",
            "step 23100: train loss 0.2274, val loss 0.2197\n",
            "step 23200: train loss 0.2423, val loss 0.2501\n",
            "step 23300: train loss 0.2658, val loss 0.2693\n",
            "step 23400: train loss 0.2481, val loss 0.2519\n",
            "step 23500: train loss 0.2430, val loss 0.2456\n",
            "step 23600: train loss 0.2076, val loss 0.2137\n",
            "step 23700: train loss 0.2097, val loss 0.2202\n",
            "step 23800: train loss 0.2584, val loss 0.2548\n",
            "step 23900: train loss 0.2038, val loss 0.2022\n",
            "step 24000: train loss 0.2006, val loss 0.2234\n",
            "step 24100: train loss 0.2193, val loss 0.2264\n",
            "step 24200: train loss 0.2119, val loss 0.2091\n",
            "step 24300: train loss 0.3263, val loss 0.3434\n",
            "step 24400: train loss 0.1902, val loss 0.1876\n",
            "step 24500: train loss 0.2790, val loss 0.2849\n",
            "step 24600: train loss 0.2029, val loss 0.2081\n",
            "step 24700: train loss 0.2992, val loss 0.3117\n",
            "step 24800: train loss 0.2259, val loss 0.2184\n",
            "step 24900: train loss 0.1821, val loss 0.1931\n",
            "step 25000: train loss 0.2267, val loss 0.2216\n",
            "step 25100: train loss 0.1951, val loss 0.1816\n",
            "step 25200: train loss 0.1753, val loss 0.1782\n",
            "step 25300: train loss 0.1772, val loss 0.1808\n",
            "step 25400: train loss 0.1776, val loss 0.1834\n",
            "step 25500: train loss 0.1700, val loss 0.1781\n",
            "step 25600: train loss 0.2013, val loss 0.2131\n",
            "step 25700: train loss 0.1813, val loss 0.1949\n",
            "step 25800: train loss 0.2061, val loss 0.1991\n",
            "step 25900: train loss 0.2044, val loss 0.2068\n",
            "step 26000: train loss 0.1629, val loss 0.1726\n",
            "step 26100: train loss 0.1724, val loss 0.1823\n",
            "step 26200: train loss 0.2098, val loss 0.1963\n",
            "step 26300: train loss 0.2165, val loss 0.2202\n",
            "step 26400: train loss 0.1822, val loss 0.1850\n",
            "step 26500: train loss 0.1658, val loss 0.1919\n",
            "step 26600: train loss 0.1913, val loss 0.1860\n",
            "step 26700: train loss 0.1591, val loss 0.1661\n",
            "step 26800: train loss 0.2205, val loss 0.2287\n",
            "step 26900: train loss 0.1582, val loss 0.1573\n",
            "step 27000: train loss 0.3801, val loss 0.3548\n",
            "step 27100: train loss 0.1539, val loss 0.1414\n",
            "step 27200: train loss 0.1666, val loss 0.1711\n",
            "step 27300: train loss 0.1514, val loss 0.1498\n",
            "step 27400: train loss 0.1710, val loss 0.1804\n",
            "step 27500: train loss 0.1501, val loss 0.1532\n",
            "step 27600: train loss 0.1717, val loss 0.1768\n",
            "step 27700: train loss 0.2023, val loss 0.1954\n",
            "step 27800: train loss 0.2217, val loss 0.2330\n",
            "step 27900: train loss 0.2071, val loss 0.2120\n",
            "step 28000: train loss 0.3126, val loss 0.3085\n",
            "step 28100: train loss 0.1694, val loss 0.1702\n",
            "step 28200: train loss 0.1486, val loss 0.1481\n",
            "step 28300: train loss 0.1471, val loss 0.1459\n",
            "step 28400: train loss 0.1759, val loss 0.1647\n",
            "step 28500: train loss 0.1596, val loss 0.1617\n",
            "step 28600: train loss 0.1570, val loss 0.1652\n",
            "step 28700: train loss 0.1609, val loss 0.1667\n",
            "step 28800: train loss 0.1616, val loss 0.1682\n",
            "step 28900: train loss 0.1375, val loss 0.1346\n",
            "step 29000: train loss 0.1976, val loss 0.1948\n",
            "step 29100: train loss 0.1701, val loss 0.1629\n",
            "step 29200: train loss 0.1516, val loss 0.1473\n",
            "step 29300: train loss 0.1509, val loss 0.1637\n",
            "step 29400: train loss 0.1599, val loss 0.1577\n",
            "step 29500: train loss 0.1590, val loss 0.1556\n",
            "step 29600: train loss 0.1507, val loss 0.1541\n",
            "step 29700: train loss 0.1177, val loss 0.1235\n",
            "step 29800: train loss 0.1690, val loss 0.1727\n",
            "step 29900: train loss 0.1575, val loss 0.1631\n",
            "step 30000: train loss 0.1428, val loss 0.1393\n",
            "step 30100: train loss 0.1404, val loss 0.1422\n",
            "step 30200: train loss 0.2017, val loss 0.1980\n",
            "step 30300: train loss 0.1316, val loss 0.1255\n",
            "step 30400: train loss 0.1758, val loss 0.1772\n",
            "step 30500: train loss 0.1345, val loss 0.1351\n",
            "step 30600: train loss 0.1234, val loss 0.1279\n",
            "step 30700: train loss 0.1195, val loss 0.1191\n",
            "step 30800: train loss 0.1388, val loss 0.1378\n",
            "step 30900: train loss 0.1341, val loss 0.1323\n",
            "step 31000: train loss 0.1425, val loss 0.1431\n",
            "step 31100: train loss 0.1374, val loss 0.1366\n",
            "step 31200: train loss 0.1490, val loss 0.1497\n",
            "step 31300: train loss 0.1491, val loss 0.1481\n",
            "step 31400: train loss 0.1154, val loss 0.1216\n",
            "step 31500: train loss 0.1242, val loss 0.1266\n",
            "step 31600: train loss 0.1145, val loss 0.1121\n",
            "step 31700: train loss 0.1370, val loss 0.1389\n",
            "step 31800: train loss 0.1225, val loss 0.1215\n",
            "step 31900: train loss 0.1478, val loss 0.1522\n",
            "step 32000: train loss 0.1653, val loss 0.1714\n",
            "step 32100: train loss 0.1160, val loss 0.1215\n",
            "step 32200: train loss 0.1061, val loss 0.1011\n",
            "step 32300: train loss 0.1111, val loss 0.1106\n",
            "step 32400: train loss 0.1449, val loss 0.1504\n",
            "step 32500: train loss 0.1564, val loss 0.1679\n",
            "step 32600: train loss 0.1379, val loss 0.1367\n",
            "step 32700: train loss 0.1204, val loss 0.1216\n",
            "step 32800: train loss 0.1126, val loss 0.1101\n",
            "step 32900: train loss 0.1242, val loss 0.1147\n",
            "step 33000: train loss 0.1476, val loss 0.1497\n",
            "step 33100: train loss 0.2047, val loss 0.2007\n",
            "step 33200: train loss 0.1278, val loss 0.1265\n",
            "step 33300: train loss 0.1460, val loss 0.1582\n",
            "step 33400: train loss 0.1094, val loss 0.1103\n",
            "step 33500: train loss 0.1072, val loss 0.1088\n",
            "step 33600: train loss 0.1377, val loss 0.1416\n",
            "step 33700: train loss 0.1249, val loss 0.1234\n",
            "step 33800: train loss 0.1384, val loss 0.1414\n",
            "step 33900: train loss 0.1077, val loss 0.1141\n",
            "step 34000: train loss 0.1213, val loss 0.1170\n",
            "step 34100: train loss 0.3003, val loss 0.2915\n",
            "step 34200: train loss 0.1145, val loss 0.1028\n",
            "step 34300: train loss 0.1094, val loss 0.1154\n",
            "step 34400: train loss 0.1161, val loss 0.1113\n",
            "step 34500: train loss 0.1212, val loss 0.1254\n",
            "step 34600: train loss 0.1385, val loss 0.1352\n",
            "step 34700: train loss 0.1420, val loss 0.1325\n",
            "step 34800: train loss 0.1268, val loss 0.1245\n",
            "step 34900: train loss 0.1199, val loss 0.1120\n",
            "step 35000: train loss 0.0967, val loss 0.0987\n",
            "step 35100: train loss 0.1231, val loss 0.1205\n",
            "step 35200: train loss 0.1307, val loss 0.1320\n",
            "step 35300: train loss 0.1069, val loss 0.1039\n",
            "step 35400: train loss 0.1076, val loss 0.1070\n",
            "step 35500: train loss 0.0920, val loss 0.0929\n",
            "step 35600: train loss 0.0936, val loss 0.0946\n",
            "step 35700: train loss 0.1068, val loss 0.1077\n",
            "step 35800: train loss 0.1013, val loss 0.0987\n",
            "step 35900: train loss 0.1214, val loss 0.1177\n",
            "step 36000: train loss 0.0928, val loss 0.0909\n",
            "step 36100: train loss 0.1621, val loss 0.1535\n",
            "step 36200: train loss 0.0986, val loss 0.1015\n",
            "step 36300: train loss 0.1187, val loss 0.1139\n",
            "step 36400: train loss 0.0985, val loss 0.1049\n",
            "step 36500: train loss 0.0948, val loss 0.0897\n",
            "step 36600: train loss 0.1097, val loss 0.1127\n",
            "step 36700: train loss 0.1274, val loss 0.1265\n",
            "step 36800: train loss 0.0855, val loss 0.0924\n",
            "step 36900: train loss 0.1255, val loss 0.1217\n",
            "step 37000: train loss 0.1257, val loss 0.1310\n",
            "step 37100: train loss 0.1071, val loss 0.1058\n",
            "step 37200: train loss 0.1141, val loss 0.1192\n",
            "step 37300: train loss 0.0954, val loss 0.0941\n",
            "step 37400: train loss 0.1063, val loss 0.1058\n",
            "step 37500: train loss 0.1692, val loss 0.1748\n",
            "step 37600: train loss 0.0933, val loss 0.0942\n",
            "step 37700: train loss 0.1072, val loss 0.1051\n",
            "step 37800: train loss 0.1175, val loss 0.1159\n",
            "step 37900: train loss 0.0884, val loss 0.0943\n",
            "step 38000: train loss 0.0853, val loss 0.0842\n",
            "step 38100: train loss 0.0868, val loss 0.0898\n",
            "step 38200: train loss 0.1243, val loss 0.1261\n",
            "step 38300: train loss 0.0941, val loss 0.0932\n",
            "step 38400: train loss 0.0881, val loss 0.0866\n",
            "step 38500: train loss 0.0954, val loss 0.0959\n",
            "step 38600: train loss 0.1066, val loss 0.1030\n",
            "step 38700: train loss 0.1340, val loss 0.1322\n",
            "step 38800: train loss 0.1206, val loss 0.1207\n",
            "step 38900: train loss 0.0974, val loss 0.1004\n",
            "step 39000: train loss 0.0893, val loss 0.0874\n",
            "step 39100: train loss 0.1001, val loss 0.1029\n",
            "step 39200: train loss 0.0915, val loss 0.0983\n",
            "step 39300: train loss 0.1061, val loss 0.1035\n",
            "step 39400: train loss 0.1013, val loss 0.0988\n",
            "step 39500: train loss 0.0773, val loss 0.0806\n",
            "step 39600: train loss 0.0854, val loss 0.0833\n",
            "step 39700: train loss 0.0972, val loss 0.0938\n",
            "step 39800: train loss 0.0865, val loss 0.0864\n",
            "step 39900: train loss 0.1438, val loss 0.1473\n",
            "step 39999: train loss 0.0898, val loss 0.0892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history_regular['train'], label='Regular Train Loss', color='blue')\n",
        "plt.plot(history_regular['val'], label='Regular Validation Loss', color='lightblue')\n",
        "\n",
        "#plt.plot(history_feel['train'], label='Feel Train Loss', color='red')\n",
        "#plt.plot(history_feel['val'], label='Feel Validation Loss', color='salmon')\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss History')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "crqoJymEh4mj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "4be2403f-b540-430c-b665-bba0ec1d6577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "context=torch.stack([torch.stack([torch.tensor([85.0, 0.0, 0.0])]).to(device)])\n",
        "generated_sequence = model_sdf.generate(context, max_new_tokens=100)[0].tolist()\n",
        "generated_sequence\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zemqMN-eoQ95",
        "outputId": "27b5de3e-ae81-497b-cec4-05bf41c81367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[85.0, 0.0, 0.0],\n",
              " [86.0, 0.0, 0.0],\n",
              " [80.0, 0.0, 0.0],\n",
              " [91.0, 0.0, 0.0],\n",
              " [77.0, 0.0, 0.0],\n",
              " [83.0, 0.0, 0.0],\n",
              " [77.0, 0.0, 0.0],\n",
              " [83.0, 0.0, 0.0],\n",
              " [80.0, 0.0, 0.0],\n",
              " [92.0, 0.0, 0.0],\n",
              " [83.0, 0.0, 0.0],\n",
              " [84.0, 0.0, 0.0],\n",
              " [89.0, 1.0, 0.0],\n",
              " [106.0, 1.0, 0.0],\n",
              " [75.0, 1.0, 0.0],\n",
              " [91.0, 1.0, 0.0],\n",
              " [85.0, 1.0, 0.0],\n",
              " [95.0, 1.0, 0.0],\n",
              " [86.0, 1.0, 0.0],\n",
              " [95.0, 1.0, 0.0],\n",
              " [79.0, 1.0, 0.0],\n",
              " [81.0, 1.0, 0.0],\n",
              " [88.0, 1.0, 0.0],\n",
              " [81.0, 1.0, 0.0],\n",
              " [87.0, 1.0, 0.0],\n",
              " [90.0, 1.0, 0.0],\n",
              " [84.0, 2.0, 0.0],\n",
              " [100.0, 2.0, 0.0],\n",
              " [93.0, 2.0, 0.0],\n",
              " [89.0, 2.0, 0.0],\n",
              " [80.0, 2.0, 0.0],\n",
              " [78.0, 2.0, 0.0],\n",
              " [76.0, 2.0, 0.0],\n",
              " [77.0, 2.0, 0.0],\n",
              " [76.0, 2.0, 0.0],\n",
              " [77.0, 2.0, 0.0],\n",
              " [76.0, 2.0, 1.0],\n",
              " [92.0, 2.0, 1.0],\n",
              " [92.0, 2.0, 1.0],\n",
              " [92.0, 2.0, 1.0],\n",
              " [92.0, 2.0, 1.0],\n",
              " [92.0, 2.0, 1.0],\n",
              " [92.0, 2.0, 1.0],\n",
              " [92.0, 2.0, 1.0],\n",
              " [92.0, 2.0, 1.0],\n",
              " [87.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [87.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 2.0, 1.0],\n",
              " [91.0, 3.0, 1.0],\n",
              " [81.0, 3.0, 1.0],\n",
              " [107.0, 3.0, 1.0],\n",
              " [107.0, 2.0, 1.0],\n",
              " [87.0, 2.0, 1.0],\n",
              " [82.0, 2.0, 1.0],\n",
              " [88.0, 3.0, 1.0],\n",
              " [88.0, 3.0, 1.0],\n",
              " [88.0, 3.0, 1.0],\n",
              " [88.0, 3.0, 1.0],\n",
              " [88.0, 3.0, 1.0],\n",
              " [83.0, 3.0, 1.0],\n",
              " [85.0, 3.0, 1.0],\n",
              " [85.0, 4.0, 1.0],\n",
              " [97.0, 5.0, 1.0],\n",
              " [107.0, 4.0, 1.0],\n",
              " [83.0, 3.0, 1.0],\n",
              " [83.0, 3.0, 1.0],\n",
              " [85.0, 4.0, 1.0],\n",
              " [97.0, 4.0, 1.0],\n",
              " [83.0, 5.0, 1.0],\n",
              " [99.0, 7.0, 1.0],\n",
              " [102.0, 7.0, 1.0],\n",
              " [91.0, 7.0, 1.0],\n",
              " [95.0, 7.0, 1.0],\n",
              " [103.0, 10.0, 1.0],\n",
              " [103.0, 10.0, 4.0],\n",
              " [99.0, 9.0, 4.0],\n",
              " [101.0, 8.0, 4.0],\n",
              " [109.0, 8.0, 4.0],\n",
              " [100.0, 7.0, 4.0],\n",
              " [109.0, 8.0, 4.0],\n",
              " [86.0, 8.0, 4.0],\n",
              " [87.0, 8.0, 4.0],\n",
              " [87.0, 8.0, 4.0],\n",
              " [99.0, 8.0, 4.0],\n",
              " [100.0, 8.0, 4.0],\n",
              " [100.0, 8.0, 4.0],\n",
              " [109.0, 8.0, 4.0],\n",
              " [100.0, 8.0, 4.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming 'model_regular' is your trained model\n",
        "with open('model_regular.pkl', 'wb') as f:\n",
        "    pickle.dump(model_regular, f)"
      ],
      "metadata": {
        "id": "o5eTcoaxvTwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sth=torch.stack([\n",
        "        torch.tensor([85, 11, 12 ]),\n",
        "        torch.tensor([95, 21, 22 ]) ,\n",
        "        torch.tensor([105, 31, 32])\n",
        "        ]\n",
        "                 ).to(device)\n",
        "\n",
        "sth.shape\n",
        "\n",
        "one=sth[:, 0:1]\n",
        "two=sth[:, 1:2]\n",
        "three=sth[:, 2:3]\n",
        "\n",
        "torch.cat([one,two, three], dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezW_FqW20r8y",
        "outputId": "5afedb44-8e51-4f8e-f1aa-b1ef24c5a9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 85,  11,  12],\n",
              "        [ 95,  21,  22],\n",
              "        [105,  31,  32]])"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}